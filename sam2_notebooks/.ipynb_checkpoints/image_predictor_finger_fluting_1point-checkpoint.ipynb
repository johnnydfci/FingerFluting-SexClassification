{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7847e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "Torchvision version: 0.20.1+cu124\n",
      "CUDA is available: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhongyi/anaconda3/envs/sam/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a15e2f-c7e1-4e5d-862f-fcb751a60b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )\n",
    "np.random.seed(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb1927b",
   "metadata": {},
   "source": [
    "First, load the SAM 2 model and predictor. Change the path below to point to the SAM 2 checkpoint. Running on CUDA and using the default model are recommended for best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b00de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\n",
    "\n",
    "predictor = SAM2ImagePredictor(sam2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d4c9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import label, find_objects, binary_erosion, binary_dilation\n",
    "from scipy.ndimage import generate_binary_structure\n",
    "\n",
    "\n",
    "def get_largest_connected_component_and_bbox_mask(mask, radius=5, dilation_radius=20):\n",
    "    # Define a structuring element for erosion and dilation\n",
    "    struct_elem = generate_binary_structure(2, 1)\n",
    "    \n",
    "    # Erode and then dilate the mask to remove small noise\n",
    "    mask_eroded = binary_erosion(mask, structure=struct_elem, iterations=radius)\n",
    "    mask_cleaned = binary_dilation(mask_eroded, structure=struct_elem, iterations=radius)\n",
    "    \n",
    "    # Label connected components in the cleaned mask\n",
    "    labeled_array, num_features = label(mask_cleaned)\n",
    "    \n",
    "    # Get sizes of connected components\n",
    "    component_sizes = [(labeled_array == i).sum() for i in range(1, num_features + 1)]\n",
    "    \n",
    "    # Identify the largest component by its label\n",
    "    largest_component_label = np.argmax(component_sizes) + 1\n",
    "    \n",
    "    # Create a mask with only the largest connected component\n",
    "    largest_component = (labeled_array == largest_component_label).astype(np.uint8)\n",
    "    \n",
    "    # Find the bounding box of the largest connected component\n",
    "    bbox = find_objects(labeled_array == largest_component_label)[0]\n",
    "    min_row, min_col = bbox[0].start, bbox[1].start\n",
    "    max_row, max_col = bbox[0].stop, bbox[1].stop\n",
    "    \n",
    "    # Create an empty array of the same shape as the input mask\n",
    "    bbox_mask = np.zeros_like(mask, dtype=np.uint8)\n",
    "    \n",
    "    # Set the bounding box region to 1\n",
    "    bbox_mask[min_row:max_row, min_col:max_col] = 1\n",
    "\n",
    "    # Dilate the bounding box mask by 20 pixels\n",
    "    bbox_mask_dilated = binary_dilation(bbox_mask, structure=struct_elem, iterations=dilation_radius)\n",
    "    \n",
    "    return largest_component, bbox_mask_dilated\n",
    "\n",
    "\n",
    "def crop_image_by_bbox_per_channel(image, bbox_mask):\n",
    "    # Find the coordinates of the bounding box from the bbox mask\n",
    "    rows = np.any(bbox_mask, axis=1)\n",
    "    cols = np.any(bbox_mask, axis=0)\n",
    "    min_row, max_row = np.where(rows)[0][[0, -1]]\n",
    "    min_col, max_col = np.where(cols)[0][[0, -1]]\n",
    "    \n",
    "    # Crop each channel of the image using the bounding box coordinates\n",
    "    cropped_channels = [image[min_row:max_row+1, min_col:max_col+1, c] for c in range(image.shape[2])]\n",
    "    \n",
    "    # Stack cropped channels back together to form the final cropped image\n",
    "    cropped_image = np.stack(cropped_channels, axis=-1)\n",
    "    \n",
    "    return cropped_image\n",
    "\n",
    "def get_center_and_center_rectangle(array, size_factor=0.2):\n",
    "    h, w = array.shape\n",
    "    cy, cx = h // 2, w // 2\n",
    "    rh, rw = int(h * size_factor), int(w * size_factor)\n",
    "    \n",
    "    # Calculate the corner points\n",
    "    top_left = [cx - rw // 2, cy - rh // 2]\n",
    "    top_right = [top_left[0], top_left[1] + rh]\n",
    "    bottom_left = [top_left[0] + rw, top_left[1]]\n",
    "    bottom_right = [top_left[0] + rw, top_left[1] + rh]\n",
    "    \n",
    "    # Return the coordinates as a list\n",
    "    return np.array([[cx, cy], top_left, top_right, bottom_left, bottom_right])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23842fb2",
   "metadata": {},
   "source": [
    "## Example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0567f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Directories for input images and output cropped images\n",
    "img_dir  = 'quarter_reslution_tactile_img_717/'  # Input directory containing raw tactile images\n",
    "crop_dir = 'quarter_reslution_tactile_img_cropped_1point/'  # Output directory for cropped images\n",
    "\n",
    "# Iterate through all files in the input directory in sorted order\n",
    "for idx, d in enumerate(sorted(os.listdir(img_dir))):\n",
    "    # Construct the full path to the input image\n",
    "    img_path = img_dir + d    \n",
    "    \n",
    "    # Modify the file name to include '_cropped' before the extension ('id001.jpg' -> 'id001_cropped.jpg')\n",
    "    dot_index = d.rfind('.')\n",
    "    crop_d = d[:dot_index] + '_cropped' + d[dot_index:]\n",
    "    targ_path = crop_dir + crop_d  # Full path for the cropped image output\n",
    "    \n",
    "    # Log the index and file name being processed\n",
    "    print(idx, d)\n",
    "    \n",
    "    # Open the input image and ensure it's in RGB format\n",
    "    image = Image.open(img_path)\n",
    "    image = np.array(image.convert(\"RGB\"))\n",
    "    \n",
    "    # Set the image for the predictor model\n",
    "    predictor.set_image(image)\n",
    "    \n",
    "    # Define a central point as the input for the model\n",
    "    input_point = np.array([[int(image.shape[1]/2), int(image.shape[0]/2)]])  # Central point of the image\n",
    "    input_label = np.array([1])  # Label for the input point (e.g., foreground)\n",
    "    \n",
    "    # Perform prediction using the SAM2 model\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        multimask_output=True,  # Generate multiple masks\n",
    "    )\n",
    "    \n",
    "    # Sort the masks by their confidence scores in descending order\n",
    "    sorted_ind = np.argsort(scores)[::-1]\n",
    "    masks = masks[sorted_ind]  # Reorder masks by scores\n",
    "    scores = scores[sorted_ind]  # Reorder scores\n",
    "    logits = logits[sorted_ind]  # Reorder logits\n",
    "    \n",
    "    # Extract the largest connected component and the bounding box mask from the highest-scoring mask\n",
    "    largest_component, bbox_mask = get_largest_connected_component_and_bbox_mask(masks[0])\n",
    "    \n",
    "    # Crop the image using the bounding box mask\n",
    "    cropped_image = crop_image_by_bbox_per_channel(image, bbox_mask)\n",
    "    \n",
    "    # Ensure the cropped image is in the correct data type for saving\n",
    "    cropped_image = (cropped_image * 255).astype(np.uint8) if cropped_image.dtype != np.uint8 else cropped_image\n",
    "    \n",
    "    # Save the cropped image to the target path\n",
    "    Image.fromarray(cropped_image).save(targ_path)\n",
    "    \n",
    "    # Log the success of the cropping operation\n",
    "    print(idx, d, \"Cropped image shape:\", cropped_image.shape, 'saved', targ_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sam]",
   "language": "python",
   "name": "conda-env-sam-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
